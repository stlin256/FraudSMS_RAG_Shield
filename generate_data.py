'''
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
â•šâ•â•â•â•â•â•â•   â•šâ•â•   â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â•
'''
import os
import json
import faiss
import numpy as np
import pandas as pd
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
import torch

# ========== é…ç½® ==========
MODEL_PATH = "./m3e-base"  # ä½¿ç”¨ m3e-large è¿›è¡Œå‘é‡åŒ–
DATA_DIR = "Telecom_Fraud_Texts_5-main"  # çŸ­ä¿¡æ•°æ®é›†è·¯å¾„
FAISS_INDEX_FILE = "fraud_sms_faiss.index"  # FAISS ç´¢å¼•æ–‡ä»¶
METADATA_FILE = "fraud_sms_metadata.json"  # çŸ­ä¿¡å…ƒæ•°æ®æ–‡ä»¶
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
GPU_DEVICE = 0  # æŒ‡å®š GPU è®¾å¤‡

# ========== åŠ è½½æœ¬åœ° m3e æ¨¡å‹ ==========
print("ğŸ“¦ æ­£åœ¨åŠ è½½ m3e-base æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModel.from_pretrained(MODEL_PATH).to(DEVICE)
model.eval()

# ========== çŸ­ä¿¡ç±»åˆ«æ˜ å°„ ==========
label_mapping = {
    "label00-last.csv": "æ­£å¸¸çŸ­ä¿¡",
    "label01-last.csv": "å†’å……å…¬æ£€æ³•",
    "label02-last.csv": "è´·æ¬¾è¯ˆéª—",
    "label03-last.csv": "å†’å……å®¢æœ",
    "label04-last.csv": "å†’å……é¢†å¯¼æˆ–ç†Ÿäººè¯ˆéª—"
}

# ========== å‘é‡åŒ–å‡½æ•° ==========
@torch.no_grad()
def get_embedding(text: str):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to(DEVICE)
    outputs = model(**inputs)
    embedding = outputs.last_hidden_state.mean(dim=1)  # å–å‡å€¼ä½œä¸ºå‘é‡è¡¨ç¤º
    return embedding.squeeze().cpu().numpy()

# ========== åˆå§‹åŒ– FAISS ç´¢å¼• ==========
print("åˆå§‹åŒ– FAISS ç´¢å¼•...")
dimension = 768  # m3e-base è¾“å‡º 768 ç»´
cpu_index = faiss.IndexFlatL2(dimension)
gpu_res = faiss.StandardGpuResources()
gpu_index = faiss.index_cpu_to_gpu(gpu_res, GPU_DEVICE, cpu_index)

# ========== è¯»å–çŸ­ä¿¡æ•°æ®å¹¶å‘é‡åŒ– ==========
print("è¯»å–çŸ­ä¿¡æ•°æ®å¹¶å‘é‡åŒ–...")
metadata = []
all_vectors = []

for file_name, label in label_mapping.items():
    file_path = os.path.join(DATA_DIR, file_name)
    df = pd.read_csv(file_path, encoding="utf-8")

    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f"ğŸ” å¤„ç† {label}"):
        text = str(row.iloc[0]).strip()  # è·å–çŸ­ä¿¡æ–‡æœ¬
        if not text:
            continue

        # è®¡ç®—æ–‡æœ¬å‘é‡
        vec = get_embedding(text)
        all_vectors.append(vec)

        # è®°å½•å…ƒæ•°æ®
        metadata.append({
            "id": f"{file_name}_{idx}",
            "label": label,
            "text": text
        })

# ========== æ·»åŠ å‘é‡åˆ°ç´¢å¼• ==========
if all_vectors:
    print("âš¡ æ­£åœ¨å°†å‘é‡æ·»åŠ åˆ° FAISS ç´¢å¼•...")
    vectors_np = np.array(all_vectors, dtype="float32")
    print("Vectors shape:", vectors_np.shape)  # æ‰“å°å‘é‡æ•°ç»„çš„å½¢çŠ¶
    gpu_index.add(vectors_np)

# ========== ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ® ==========
print("ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®...")
index_cpu = faiss.index_gpu_to_cpu(gpu_index)
faiss.write_index(index_cpu, FAISS_INDEX_FILE)

with open(METADATA_FILE, "w", encoding="utf-8") as f:
    json.dump(metadata, f, ensure_ascii=False, indent=2)

print(f"\n å…±æ·»åŠ çŸ­ä¿¡å‘é‡ï¼š{len(metadata)}")
print(f" å‘é‡ç´¢å¼•ä¿å­˜è‡³ï¼š{FAISS_INDEX_FILE}")
print(f" å…ƒæ•°æ®ä¿å­˜è‡³ï¼š{METADATA_FILE}")
