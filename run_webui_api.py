'''
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
â•šâ•â•â•â•â•â•â•   â•šâ•â•   â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â•
'''
import json
import faiss
import torch
from transformers import AutoTokenizer
from transformers import AutoModel, StoppingCriteria
import gradio as gr
from openai import OpenAI
import time

# ========== é…ç½® ==========
M3E_PATH = "./m3e-base"  # m3e å‘é‡åŒ–æ¨¡å‹è·¯å¾„
FAISS_INDEX_FILE = "fraud_sms_faiss.index"  # FAISS ç´¢å¼•æ–‡ä»¶
METADATA_FILE = "fraud_sms_metadata.json"  # å…ƒæ•°æ®æ–‡ä»¶
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"  # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
MAX_TOKENS = 1024  # é™åˆ¶æ¨¡å‹æœ€å¤§è¾“å‡ºé•¿åº¦
api_key = "your_api_key" #ä½ çš„api_key
base_url = "your_base_url" #ä½ çš„base_url
model_name = "your_model" #ä½ è°ƒç”¨çš„æ¨¡å‹
DE_BUG = True  # æ˜¾ç¤ºæ¨¡å‹åŸå§‹è¾“å‡ºå¼€å…³
SHOW_CATEGORY = True  # è¾“å‡ºç”„åˆ«ç±»åˆ«å¼€å…³
SHOW_SAMPLE = True  # è¾“å‡ºç›¸ä¼¼çŸ­ä¿¡å¼€å…³
SHOW_HISTORY = False #åœ¨ç»ˆç«¯è¾“å‡ºå¯¹è¯å†å²è®°å½•
SHOW_USED_TIME = True #åœ¨ç»ˆç«¯è¾“å‡ºé—®ç­”æ‰€ç”¨æ—¶é—´

client = OpenAI(api_key=api_key, base_url=base_url)

# ========== åŠ è½½ FAISS ç´¢å¼• ==========
print("åŠ è½½ FAISS ç´¢å¼•...")
index = faiss.read_index(FAISS_INDEX_FILE)
with open(METADATA_FILE, "r", encoding="utf-8") as f:
    metadata = json.load(f)

# ========== å®šä¹‰çŸ­ä¿¡ç±»åˆ« ==========
LABELS = [
    "æ­£å¸¸çŸ­ä¿¡",
    "å†’å……å…¬æ£€æ³•",
    "è´·æ¬¾è¯ˆéª—",
    "å†’å……å®¢æœ",
    "å†’å……é¢†å¯¼æˆ–ç†Ÿäººè¯ˆéª—"
]

# ========== åŠ è½½ m3e å‘é‡åŒ–æ¨¡å‹ ==========
print("åŠ è½½ m3e å‘é‡åŒ–æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(M3E_PATH)
model = AutoModel.from_pretrained(M3E_PATH).to(DEVICE).eval()


@torch.no_grad()
def get_embedding(text: str):
    """è·å–æ–‡æœ¬çš„ m3e åµŒå…¥å‘é‡"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to(DEVICE)
    outputs = model(**inputs)
    embedding = outputs.last_hidden_state.mean(dim=1)
    return embedding.squeeze().cpu().numpy()


# ========== RAG æ£€ç´¢ ==========
def retrieve_similar_texts(query, top_k=5):
    """æ£€ç´¢ä¸æŸ¥è¯¢çŸ­ä¿¡æœ€ç›¸ä¼¼çš„ top_k æ¡çŸ­ä¿¡"""
    query_vec = get_embedding(query).reshape(1, -1)
    distances, indices = index.search(query_vec, top_k)
    results = [metadata[idx] for idx in indices[0] if idx < len(metadata)]
    return results


# ========== è‡ªå®šä¹‰åœæ­¢æ¡ä»¶ ==========
class DynamicStoppingCriteria(StoppingCriteria):
    """åŠ¨æ€åœæ­¢æ¡ä»¶ï¼šå½“ç”Ÿæˆæœ‰æ•ˆ ``` å—æ—¶åœæ­¢"""

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, input_ids, scores, **kwargs):
        decoded_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
        if "```" in decoded_text:
            blocks = decoded_text.split("```")
            for i in range(1, len(blocks), 2):  # æ£€æŸ¥æ¯ä¸ªå†…å®¹å—
                block = blocks[i].strip()
                if block:
                    lines = block.split("\n")
                    has_label = False
                    has_reason = False
                    label_valid = False
                    reason_valid = False
                    for line in lines:
                        line = line.strip()
                        if line.startswith("é¢„æµ‹ç±»åˆ«:"):
                            has_label = True
                            label = line.split("é¢„æµ‹ç±»åˆ«:")[1].strip()
                            label_valid = "<ç±»åˆ«åç§°>" not in label and label != "æœªçŸ¥" and label
                        elif line.startswith("ç†ç”±:"):
                            has_reason = True
                            reason = line.split("ç†ç”±:")[1].strip()
                            reason_valid = "<ç®€çŸ­ç†ç”±" not in reason and reason != "æœªæä¾›ç†ç”±" and reason
                    if has_label and has_reason and label_valid and reason_valid and decoded_text.endswith("```"):
                        return True
        return False


# ========== å•æ¬¡åˆ†ç±»å‡½æ•° ==========
def single_classify(sms_text, similar_texts, attempt_num):
    """å•æ¬¡çŸ­ä¿¡åˆ†ç±»"""
    retrieved_examples = "\n".join(
        [f"- çŸ­ä¿¡: {item['text']} | ç±»åˆ«: {item['label']}" for item in similar_texts]
    ) if (similar_texts and similar_texts != " ") else "æ— ç›¸ä¼¼çŸ­ä¿¡ç¤ºä¾‹ã€‚"

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„æ™ºèƒ½çŸ­ä¿¡åˆ†ç±»åŠ©æ‰‹ï¼Œæ“…é•¿è¯ˆéª—çŸ­ä¿¡ç”„åˆ«ã€‚è¯·ä¾æ®ä»¥ä¸‹åˆ†ç±»æ­¥éª¤ç”Ÿæˆä¸€ä¸ªæœ‰æ•ˆçš„ ``` å—ç»“æœï¼Œä¸è¾“å‡ºå¤šä½™ä¿¡æ¯ã€‚ç¡®ä¿ä»¥ ``` å¼€å§‹å’Œç»“æŸï¼ŒåŒ…å«â€œé¢„æµ‹ç±»åˆ«:â€ã€â€œç†ç”±:â€ï¼Œä¸å«å ä½ç¬¦ï¼ˆå¦‚ <ç±»åˆ«åç§°>ï¼‰ã€‚ç”Ÿæˆç¬¬ä¸€ä¸ªæœ‰æ•ˆå—åç«‹å³åœæ­¢ã€‚

    åˆ†ç±»æ­¥éª¤ï¼š
    1. åˆ†æçŸ­ä¿¡ç‰¹å¾ï¼šå…³æ³¨è¯­æ°”ï¼ˆæ­£å¼/éšæ„ï¼‰ã€ç”¨è¯ï¼ˆæ˜¯å¦æ¶‰åŠé‡‘é’±ã€ç´§æ€¥ã€èº«ä»½å†’å……ï¼‰ã€æ˜¯å¦æœ‰æ•æ„Ÿè¡ŒåŠ¨è¦æ±‚ï¼ˆå¦‚è½¬è´¦ã€æä¾›ä¿¡æ¯ï¼‰ã€‚
    2. â€œå†’å……å®¢æœâ€è¯†åˆ«ï¼šæ³¨æ„å‡å†’å®¢æœå¸¸ä½¿ç”¨ä¼ªä¸“ä¸šè¯­æ°”ã€è¦æ±‚ç‚¹å‡»é“¾æ¥éªŒè¯èº«ä»½æˆ–æ±‡æ¬¾ç­‰æ“ä½œï¼Œè€Œæ­£å¸¸å®¢æœè¯­æ°”å¹³ç¨³ã€æ— æ•æ„Ÿç´¢å–ï¼Œéœ€è¦æ³¨æ„å’Œå†’å……å…¬æ£€æ³•ã€å†’å……é¢†å¯¼æˆ–ç†Ÿäººè¯ˆéª—ã€æ­£å¸¸çŸ­ä¿¡çš„åŒºåˆ†
    3. â€œå†’å……é¢†å¯¼æˆ–ç†Ÿäººè¯ˆéª—â€å…·æœ‰æ˜ç¡®çš„èµ„é‡‘æˆ–æ•æ„Ÿæ•°æ®è¦æ±‚ï¼Œå°†å…¶ä¸â€œæ­£å¸¸çŸ­ä¿¡â€åŒºåˆ†å¼€ï¼Œå•çº¯è¾±éª‚çš„çŸ­ä¿¡ä¸å±äºè¯ˆéª—ã€‚
    4. å‚è€ƒç›¸ä¼¼çŸ­ä¿¡ï¼šç»“åˆç¤ºä¾‹çš„ç±»åˆ«åˆ†å¸ƒï¼Œè¯„ä¼°è¾“å…¥çŸ­ä¿¡ä¸ç¤ºä¾‹çš„ç›¸ä¼¼æ€§ã€‚
    5. é€»è¾‘æ¨ç†ï¼šåŸºäºç‰¹å¾å’Œç¤ºä¾‹ï¼Œå¾—å‡ºæœ€å¯èƒ½çš„ç±»åˆ«ã€‚

    è¾“å…¥ä¿¡æ¯ï¼š
    çŸ­ä¿¡å†…å®¹: "{sms_text}"
    ç›¸ä¼¼çŸ­ä¿¡ç¤ºä¾‹ï¼š
    {retrieved_examples}

    çŸ­ä¿¡ç±»åˆ«é€‰é¡¹ï¼š
    1. æ­£å¸¸çŸ­ä¿¡ï¼ˆæ—¥å¸¸é€šçŸ¥ã€æœåŠ¡ç¡®è®¤ã€ä¿ƒé”€ç­‰ï¼‰
    2. å†’å……å…¬æ£€æ³•ï¼ˆå‡è£…æ‰§æ³•æœºæ„ï¼Œæå“æˆ–ç´¢è¦ä¿¡æ¯ï¼‰
    3. è´·æ¬¾è¯ˆéª—ï¼ˆæ¶‰åŠè´·æ¬¾ã€ä¼˜æƒ æ¡ä»¶è¯±å¯¼è½¬è´¦ï¼‰
    4. å†’å……å®¢æœï¼ˆå‡å†’æœåŠ¡äººå‘˜ç´¢è¦ä¿¡æ¯æˆ–é’±è´¢ï¼‰
    5. å†’å……é¢†å¯¼æˆ–ç†Ÿäººè¯ˆéª—ï¼ˆå‡è£…ç†Ÿäººæˆ–ä¸Šçº§è¦æ±‚è½¬è´¦ï¼‰

    è¾“å‡ºæ ¼å¼ï¼š
    ```
    é¢„æµ‹ç±»åˆ«: <ç±»åˆ«åç§°>
    ç†ç”±: <ç®€çŸ­ç†ç”±ï¼ˆ50å­—ä»¥å†…ï¼‰ï¼Œè¯´æ˜ç‰¹å¾å’Œæ¨ç†ä¾æ®>
    ```
    """

    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„æ™ºèƒ½çŸ­ä¿¡åˆ†ç±»åŠ©æ‰‹ï¼Œæ“…é•¿è¯ˆéª—çŸ­ä¿¡ç”„åˆ«ã€‚"},
            {"role": "user", "content": prompt}
        ],
        max_tokens=MAX_TOKENS,
        stream=False
    )
    response = response.choices[0].message.content.strip()
    if DE_BUG:
        print(response)

    pred_label = "æœªçŸ¥"
    reason = "æœªæä¾›ç†ç”±"
    similar_sms = "æ— "

    if "```" in response:
        blocks = response.split("```")
        for i in range(1, len(blocks), 2):
            block = blocks[i].strip()
            if block:
                lines = block.split("\n")
                temp_label = "æœªçŸ¥"
                temp_reason = "æœªæä¾›ç†ç”±"
                for line in lines:
                    line = line.strip()
                    if line.startswith("é¢„æµ‹ç±»åˆ«:"):
                        temp_label = line.split("é¢„æµ‹ç±»åˆ«:")[1].strip()
                    elif line.startswith("ç†ç”±:"):
                        temp_reason = line.split("ç†ç”±:")[1].strip()
                if ("<ç±»åˆ«åç§°>" not in temp_label and
                        "<ç®€çŸ­ç†ç”±" not in temp_reason and
                        temp_label != "æœªçŸ¥" and
                        temp_reason != "æœªæä¾›ç†ç”±"):
                    pred_label = temp_label
                    reason = temp_reason
                    for item in similar_texts:
                        if item["label"] == pred_label:
                            similar_sms = item["text"]
                            break
                    break

        if pred_label == "æœªçŸ¥" or reason == "æœªæä¾›ç†ç”±":
                if DE_BUG:
                    print(f"âŒæ¨¡å‹è¾“å‡ºæ— æ•ˆ")
                return None, None, None
        else:
            return pred_label, reason, similar_sms


# ========== è¿›è¡Œåˆ†ç±» ==========
def classify_sms(sms_text, history=None):
    """ä¸»åˆ†ç±»å‡½æ•°"""

    start_time = time.time()

    if not sms_text.strip():
        return "è¯·è¾“å…¥çŸ­ä¿¡å†…å®¹ï¼", history or []

    if history is None:
        history = []

    similar_texts = retrieve_similar_texts(sms_text)
    if not similar_texts and DE_BUG:
        print("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸ä¼¼çŸ­ä¿¡ï¼Œä½¿ç”¨é»˜è®¤é€»è¾‘åˆ†ç±»ã€‚")

    output_lines = []
    pred_label, reason, similar_sms = single_classify(sms_text, similar_texts, 1)
    if pred_label is None:
        output_lines.append("âŒ æ¨¡å‹è¾“å‡ºå¼‚å¸¸")
    else:
        if pred_label != 'æ­£å¸¸çŸ­ä¿¡':
            output_lines.append('ğŸ›‘ è­¦å‘Šï¼Œè¿™æ˜¯è¯ˆéª—çŸ­ä¿¡â—â—â—')
        else:
            output_lines.append('ğŸŸ¢æ­£å¸¸çŸ­ä¿¡')
        if SHOW_CATEGORY:
            output_lines.append(f"ğŸ¯ ç±»åˆ«: {pred_label}")
        output_lines.append(f"ç†ç”±: {reason}")
        if SHOW_SAMPLE:
            output_lines.append(f"ç›¸ä¼¼{'è¯ˆéª—' if pred_label != 'æ­£å¸¸çŸ­ä¿¡' else ''}çŸ­ä¿¡: {similar_sms}")
    output = "\n".join(output_lines)
    history.append({"role": "user", "content": sms_text})
    history.append({"role": "assistant", "content": output})
    if SHOW_HISTORY:
        print("History:", history)
    if SHOW_USED_TIME:
        used_time = format(time.time()-start_time, '.2f')
        print(f"æœ¬æ¬¡å¯¹è¯ç”¨æ—¶{used_time}ç§’")
    return output, history


# ========== Gradio ç•Œé¢ ==========
def create_interface():
    """åˆ›å»º Gradio å¯è§†åŒ–ç•Œé¢"""
    mode_info =f"""
    By: stlin256 
    å½“å‰è°ƒç”¨æ¨¡å‹ï¼š{model_name}
    """

    with gr.Blocks(title="è¯ˆéª—çŸ­ä¿¡ç”„åˆ«ç³»ç»Ÿ") as demo:
        gr.Markdown("# è¯ˆéª—çŸ­ä¿¡ç”„åˆ«ç³»ç»Ÿ")
        gr.Markdown(mode_info)

        chatbot = gr.Chatbot(label="å¯¹è¯å†å²", type="messages", height=300, value=[
            {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯è¯ˆéª—çŸ­ä¿¡ç”„åˆ«åŠ©æ‰‹ï¼Œè¯·è¾“å…¥çŸ­ä¿¡å†…å®¹ï¼Œæˆ‘ä¼šå¸®åŠ©ä½ åˆ¤æ–­æ˜¯å¦æ˜¯è¯ˆéª—çŸ­ä¿¡ã€‚"}
        ])
        state = gr.State(value=[{"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯è¯ˆéª—çŸ­ä¿¡ç”„åˆ«åŠ©æ‰‹ï¼Œè¯·è¾“å…¥çŸ­ä¿¡å†…å®¹ï¼Œæˆ‘ä¼šå¸®åŠ©ä½ åˆ¤æ–­æ˜¯å¦æ˜¯è¯ˆéª—çŸ­ä¿¡ã€‚"}])

        with gr.Row():
            sms_input = gr.Textbox(label="è¯·è¾“å…¥è¦åˆ†ç±»çš„çŸ­ä¿¡", placeholder="åœ¨æ­¤è¾“å…¥çŸ­ä¿¡å†…å®¹...")
            submit_btn = gr.Button("æäº¤")

        output_text = gr.Textbox(label="åˆ†ç±»ç»“æœ", interactive=False)

        submit_btn.click(
            fn=classify_sms,
            inputs=[sms_input, state],
            outputs=[output_text, chatbot]
        )

        clear_btn = gr.Button("æ¸…ç©ºå†å²")
        clear_btn.click(
            fn=lambda: ("", []),
            inputs=None,
            outputs=[output_text, chatbot]
        )

    return demo


# ========== ä¸»å‡½æ•° ==========
if __name__ == "__main__":
    interface = create_interface()
    interface.launch()