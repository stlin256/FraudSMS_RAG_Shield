'''
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
â•šâ•â•â•â•â•â•â•   â•šâ•â•   â•šâ•â•â•â•â•â•â•â•šâ•â•â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â•
'''
import json
import faiss
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from transformers import AutoModel, StoppingCriteria, StoppingCriteriaList

# ========== é…ç½® ==========
LLM_PATH = "./Qwen2.5-7B"
M3E_PATH = "./m3e-base"
FAISS_INDEX_FILE = "fraud_sms_faiss.index"
METADATA_FILE = "fraud_sms_metadata.json"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SAFE_MODE = False  # True: ç¨³å¦¥æ¨¡å¼ï¼ˆä¸‰æ¬¡é¢„æµ‹å–å¤šæ•°ï¼‰ï¼ŒFalse: æ™®é€šæ¨¡å¼ï¼ˆå•æ¬¡é¢„æµ‹ï¼‰
DE_BUG = False #æ˜¾ç¤ºæ¨¡å‹åŸå§‹è¾“å‡ºå¼€å…³
SHOW_CATEGORY = False #è¾“å‡ºç”„åˆ«ç±»åˆ«å¼€å…³
SHOW_SAMPLE = True #è¾“å‡ºç›¸ä¼¼çŸ­ä¿¡å¼€å…³
MAX_TOKENS = 1024 #æ¨¡å‹æœ€å¤§è¾“å‡ºé•¿åº¦
MAX_RETRIES = 2 #è¾“å‡ºå¼‚å¸¸æœ€å¤§é‡è¯•æ•°

# ========== åŠ è½½ Qwen2.5-7B ========== #ä½¿ç”¨INT4é‡åŒ–ï¼Œå¯åœ¨8Gæ˜¾å­˜æ˜¾å¡ä¸Šè¿è¡Œ
print("åŠ è½½ Qwen2.5-7B...") 
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)
llm_tokenizer = AutoTokenizer.from_pretrained(LLM_PATH, trust_remote_code=True)
llm_model = AutoModelForCausalLM.from_pretrained(
    LLM_PATH,
    quantization_config=quant_config,
    device_map="auto",
    torch_dtype=torch.float16
).eval()

# ========== åŠ è½½ FAISS ç´¢å¼• ==========
print("åŠ è½½ FAISS ç´¢å¼•...")
index = faiss.read_index(FAISS_INDEX_FILE)
with open(METADATA_FILE, "r", encoding="utf-8") as f:
    metadata = json.load(f)

# ========== å®šä¹‰çŸ­ä¿¡ç±»åˆ« ==========
LABELS = [
    "æ­£å¸¸çŸ­ä¿¡",
    "å†’å……å…¬æ£€æ³•",
    "è´·æ¬¾è¯ˆéª—",
    "å†’å……å®¢æœ",
    "å†’å……é¢†å¯¼æˆ–ç†Ÿäººè¯ˆéª—"
]

# ========== åŠ è½½ m3e å‘é‡åŒ–æ¨¡å‹ ==========
print("åŠ è½½ m3e å‘é‡åŒ–æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(M3E_PATH)
model = AutoModel.from_pretrained(M3E_PATH).to(DEVICE).eval()

@torch.no_grad()
def get_embedding(text: str):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512).to(DEVICE)
    outputs = model(**inputs)
    embedding = outputs.last_hidden_state.mean(dim=1)
    return embedding.squeeze().cpu().numpy()

# ========== RAG æ£€ç´¢ ==========
def retrieve_similar_texts(query, top_k=5):
    query_vec = get_embedding(query).reshape(1, -1)
    distances, indices = index.search(query_vec, top_k)
    results = [metadata[idx] for idx in indices[0] if idx < len(metadata)]
    return results

# ========== è‡ªå®šä¹‰åœæ­¢æ¡ä»¶ ==========
class DynamicStoppingCriteria(StoppingCriteria):
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, input_ids, scores, **kwargs):
        decoded_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
        if "```" in decoded_text:
            blocks = decoded_text.split("```")
            # æ»šåŠ¨æ£€æŸ¥æ¯ä¸ªå¯èƒ½çš„å®Œæ•´å—ï¼ˆå¥‡æ•°ç´¢å¼•æ˜¯å†…å®¹ï¼‰
            for i in range(1, len(blocks), 2):  # æ­¥é•¿2ï¼Œè·³è¿‡éå†…å®¹éƒ¨åˆ†
                block = blocks[i].strip()
                if block:  # ç¡®ä¿å—éç©º
                    lines = block.split("\n")
                    has_label = False
                    has_reason = False
                    label_valid = False
                    reason_valid = False
                    for line in lines:
                        line = line.strip()
                        if line.startswith("é¢„æµ‹ç±»åˆ«:"):
                            has_label = True
                            label = line.split("é¢„æµ‹ç±»åˆ«:")[1].strip()
                            label_valid = "<ç±»åˆ«åç§°>" not in label and label != "æœªçŸ¥" and label
                        elif line.startswith("ç†ç”±:"):
                            has_reason = True
                            reason = line.split("ç†ç”±:")[1].strip()
                            reason_valid = "<ç®€çŸ­ç†ç”±" not in reason and reason != "æœªæä¾›ç†ç”±" and reason
                    # ç¡®ä¿å—å®Œæ•´ï¼šæœ‰æœ‰æ•ˆç±»åˆ«å’Œç†ç”±ï¼Œä¸”æ–‡æœ¬ä»¥ ``` ç»“æŸ
                    if has_label and has_reason and label_valid and reason_valid and decoded_text.endswith("```"):
                        return True
        return False

# ========== å•æ¬¡åˆ†ç±»å‡½æ•° ==========
def single_classify(sms_text, similar_texts, attempt_num):
    retrieved_examples = "\n".join(
        [f"- çŸ­ä¿¡: {item['text']} | ç±»åˆ«: {item['label']}" for item in similar_texts]
    ) if (similar_texts and similar_texts != " ") else "æ— ç›¸ä¼¼çŸ­ä¿¡ç¤ºä¾‹ã€‚"

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„æ™ºèƒ½çŸ­ä¿¡åˆ†ç±»åŠ©æ‰‹ï¼Œæ“…é•¿è¯ˆéª—çŸ­ä¿¡ç”„åˆ«ã€‚è¯·ä¾æ®ä»¥ä¸‹åˆ†ç±»æ­¥éª¤ç”Ÿæˆä¸€ä¸ªæœ‰æ•ˆçš„ ``` å—ç»“æœï¼Œä¸è¾“å‡ºå¤šä½™ä¿¡æ¯ã€‚ç¡®ä¿ä»¥ ``` å¼€å§‹å’Œç»“æŸï¼ŒåŒ…å«â€œé¢„æµ‹ç±»åˆ«:â€ã€â€œç†ç”±:â€ï¼Œä¸å«å ä½ç¬¦ï¼ˆå¦‚ <ç±»åˆ«åç§°>ï¼‰ã€‚ç”Ÿæˆç¬¬ä¸€ä¸ªæœ‰æ•ˆå—åç«‹å³åœæ­¢ã€‚

    åˆ†ç±»æ­¥éª¤ï¼š
    1. åˆ†æçŸ­ä¿¡ç‰¹å¾ï¼šå…³æ³¨è¯­æ°”ï¼ˆæ­£å¼/éšæ„ï¼‰ã€ç”¨è¯ï¼ˆæ˜¯å¦æ¶‰åŠé‡‘é’±ã€ç´§æ€¥ã€èº«ä»½å†’å……ï¼‰ã€æ˜¯å¦æœ‰æ•æ„Ÿè¡ŒåŠ¨è¦æ±‚ï¼ˆå¦‚è½¬è´¦ã€æä¾›ä¿¡æ¯ï¼‰ã€‚
    2. â€œå†’å……å®¢æœâ€è¯†åˆ«ï¼šæ³¨æ„å‡å†’å®¢æœå¸¸ä½¿ç”¨ä¼ªä¸“ä¸šè¯­æ°”ã€è¦æ±‚ç‚¹å‡»é“¾æ¥éªŒè¯èº«ä»½æˆ–æ±‡æ¬¾ç­‰æ“ä½œï¼Œè€Œæ­£å¸¸å®¢æœè¯­æ°”å¹³ç¨³ã€æ— æ•æ„Ÿç´¢å–ï¼Œéœ€è¦æ³¨æ„å’Œå†’å……å…¬æ£€æ³•ã€å†’å……é¢†å¯¼æˆ–ç†Ÿäººè¯ˆéª—ã€æ­£å¸¸çŸ­ä¿¡çš„åŒºåˆ†
    3. â€œå†’å……é¢†å¯¼æˆ–ç†Ÿäººè¯ˆéª—â€å…·æœ‰æ˜ç¡®çš„èµ„é‡‘æˆ–æ•æ„Ÿæ•°æ®è¦æ±‚ï¼Œå°†å…¶ä¸â€œæ­£å¸¸çŸ­ä¿¡â€åŒºåˆ†å¼€ï¼Œå•çº¯è¾±éª‚çš„çŸ­ä¿¡ä¸å±äºè¯ˆéª—ã€‚
    4. å‚è€ƒç›¸ä¼¼çŸ­ä¿¡ï¼šç»“åˆç¤ºä¾‹çš„ç±»åˆ«åˆ†å¸ƒï¼Œè¯„ä¼°è¾“å…¥çŸ­ä¿¡ä¸ç¤ºä¾‹çš„ç›¸ä¼¼æ€§ã€‚
    5. é€»è¾‘æ¨ç†ï¼šåŸºäºç‰¹å¾å’Œç¤ºä¾‹ï¼Œå¾—å‡ºæœ€å¯èƒ½çš„ç±»åˆ«ã€‚

    è¾“å…¥ä¿¡æ¯ï¼š
    çŸ­ä¿¡å†…å®¹: "{sms_text}"
    ç›¸ä¼¼çŸ­ä¿¡ç¤ºä¾‹ï¼š
    {retrieved_examples}

    çŸ­ä¿¡ç±»åˆ«é€‰é¡¹ï¼š
    1. æ­£å¸¸çŸ­ä¿¡ï¼ˆæ—¥å¸¸é€šçŸ¥ã€æœåŠ¡ç¡®è®¤ã€ä¿ƒé”€ç­‰ï¼‰
    2. å†’å……å…¬æ£€æ³•ï¼ˆå‡è£…æ‰§æ³•æœºæ„ï¼Œæå“æˆ–ç´¢è¦ä¿¡æ¯ï¼‰
    3. è´·æ¬¾è¯ˆéª—ï¼ˆæ¶‰åŠè´·æ¬¾ã€ä¼˜æƒ æ¡ä»¶è¯±å¯¼è½¬è´¦ï¼‰
    4. å†’å……å®¢æœï¼ˆå‡å†’æœåŠ¡äººå‘˜ç´¢è¦ä¿¡æ¯æˆ–é’±è´¢ï¼‰
    5. å†’å……é¢†å¯¼æˆ–ç†Ÿäººè¯ˆéª—ï¼ˆå‡è£…ç†Ÿäººæˆ–ä¸Šçº§è¦æ±‚è½¬è´¦ï¼‰

    è¾“å‡ºæ ¼å¼ï¼š
    ```
    é¢„æµ‹ç±»åˆ«: <ç±»åˆ«åç§°>
    ç†ç”±: <ç®€çŸ­ç†ç”±ï¼ˆ50å­—ä»¥å†…ï¼‰ï¼Œè¯´æ˜ç‰¹å¾å’Œæ¨ç†ä¾æ®>
    ```
    """
    
    max_retries = MAX_RETRIES
    max_tokens = MAX_TOKENS
    stopping_criteria = StoppingCriteriaList([DynamicStoppingCriteria(llm_tokenizer)])

    for retry in range(max_retries + 1):
        inputs = llm_tokenizer(prompt, return_tensors="pt").to(DEVICE)
        outputs = llm_model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=False,
            pad_token_id=llm_tokenizer.eos_token_id,
            stopping_criteria=stopping_criteria
        )
        response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
        if DE_BUG:
            print(f"ğŸ“ å°è¯• {attempt_num} - é‡è¯• {retry + 1} æ¨¡å‹è¾“å‡ºï¼š")
            print(response)

        # æå–é€»è¾‘ï¼šæå–æœ‰æ•ˆ ``` å—
        pred_label = "æœªçŸ¥"
        reason = "æœªæä¾›ç†ç”±"
        similar_sms = "æ— "

        if "```" in response:
            blocks = response.split("```")
            for i in range(1, len(blocks), 2):  # æ­¥é•¿2ï¼Œæå–å†…å®¹å—
                block = blocks[i].strip()
                if block:  # ç¡®ä¿å—éç©º
                    lines = block.split("\n")
                    temp_label = "æœªçŸ¥"
                    temp_reason = "æœªæä¾›ç†ç”±"
                    for line in lines:
                        line = line.strip()
                        if line.startswith("é¢„æµ‹ç±»åˆ«:"):
                            temp_label = line.split("é¢„æµ‹ç±»åˆ«:")[1].strip()
                        elif line.startswith("ç†ç”±:"):
                            temp_reason = line.split("ç†ç”±:")[1].strip()
                    if ("<ç±»åˆ«åç§°>" not in temp_label and
                        "<ç®€çŸ­ç†ç”±" not in temp_reason and
                        temp_label != "æœªçŸ¥" and
                        temp_reason != "æœªæä¾›ç†ç”±"):  # ç¡®ä¿éæ¨¡æ¿ä¸”æœ‰æ•ˆ
                        pred_label = temp_label
                        reason = temp_reason
                        # ä» similar_texts ä¸­é€‰æ‹©ç¬¬ä¸€æ¡ç±»å‹åŒ¹é…çš„çŸ­ä¿¡
                        for item in similar_texts:
                            if item["label"] == pred_label:
                                similar_sms = item["text"]
                                break
                        break  # æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆå—ååœæ­¢

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ¨¡æ¿æˆ–æ— æ•ˆè¾“å‡º
        if pred_label == "æœªçŸ¥" or reason == "æœªæä¾›ç†ç”±":
            if retry < max_retries:
                print(f"âš ï¸ å°è¯• {attempt_num} - é‡è¯• {retry + 1} è¾“å‡ºä¸ºæ¨¡æ¿æˆ–æ— æ•ˆï¼Œé‡è¯•ä¸­...")
                continue
            else:
                print(f"âŒ å°è¯• {attempt_num} - æ¨¡å‹å¼‚å¸¸ï¼Œé‡è¯• {max_retries} æ¬¡åä»æ— æ•ˆ")
                return None, None, None
        else:
            return pred_label, reason, similar_sms

# ========== Qwen2.5 è¿›è¡Œåˆ†ç±» ==========
def classify_sms(sms_text):
    if DE_BUG:
        print(f"ğŸ” è¾“å…¥çŸ­ä¿¡: {sms_text}")

    similar_texts = retrieve_similar_texts(sms_text)
    if not similar_texts:
        print("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸ä¼¼çŸ­ä¿¡ï¼Œä½¿ç”¨é»˜è®¤é€»è¾‘åˆ†ç±»ã€‚")

    if not SAFE_MODE:
        pred_label, reason, similar_sms = single_classify(sms_text, similar_texts, 1)
        if pred_label is None:
            print("âŒ æ¨¡å‹å¼‚å¸¸ï¼Œæ— æ³•é¢„æµ‹æ­¤çŸ­ä¿¡ï¼ˆé‡è¯•ä¸¤æ¬¡åä»ä¸ºæ¨¡æ¿è¾“å‡ºï¼‰")
            return None

        if pred_label != 'æ­£å¸¸çŸ­ä¿¡':
            print('ğŸ›‘è­¦å‘Šï¼Œè¿™æ˜¯è¯ˆéª—çŸ­ä¿¡â—â—â—')
            if SHOW_CATEGORY:
                print(f"ğŸ¯ç±»åˆ«: {pred_label}")
        else:
            print(f"ğŸ¯ç±»åˆ«: {pred_label}")
        print(f"ç†ç”±: {reason}")
        if SHOW_SAMPLE:
            print(f"ç›¸ä¼¼{'è¯ˆéª—' if pred_label != 'æ­£å¸¸çŸ­ä¿¡' else ''}çŸ­ä¿¡: {similar_sms}")
        return pred_label

    else:
        results = []
        for attempt in range(3):
            pred_label, reason, similar_sms = single_classify(sms_text, similar_texts, attempt + 1)
            if pred_label is None:
                print("âŒ æ¨¡å‹å¼‚å¸¸ï¼Œæ— æ³•é¢„æµ‹æ­¤çŸ­ä¿¡ï¼ˆç¨³å¦¥æ¨¡å¼ä¸­é‡è¯•å¼‚å¸¸ï¼‰")
                return None
            results.append((pred_label, reason, similar_sms))

            if attempt == 1 and results[0][0] == results[1][0]:
                if pred_label != 'æ­£å¸¸çŸ­ä¿¡':
                    print('ğŸ›‘è­¦å‘Šï¼Œè¿™æ˜¯è¯ˆéª—çŸ­ä¿¡â—â—â—')
                print(f"ğŸ¯ç±»åˆ«: {pred_label}")
                print(f"ç†ç”±: {reason}")
                print(f"ç›¸ä¼¼{'è¯ˆéª—' if pred_label != 'æ­£å¸¸çŸ­ä¿¡' else ''}çŸ­ä¿¡: {similar_sms}")
                return results[0][0]
            elif attempt == 2:
                if results[0][0] != results[1][0] and results[1][0] != results[2][0] and results[0][0] != results[2][0]:
                    print("â“ ç¨³å¦¥æ¨¡å¼ä¸‹ä¸‰æ¬¡é¢„æµ‹ä¸ä¸€è‡´ï¼Œæ— æ³•åˆ¤æ–­")
                    return None

                final_label = max(set([r[0] for r in results]), key=[r[0] for r in results].count)
                final_result = next(r for r in results if r[0] == final_label)
                if final_label != 'æ­£å¸¸çŸ­ä¿¡':
                    print('ğŸ›‘è­¦å‘Šï¼Œè¿™æ˜¯è¯ˆéª—çŸ­ä¿¡â—â—â—')
                    if SHOW_CATEGORY:
                        print(f"ğŸ¯ç±»åˆ«: {final_label}")
                else:
                    print(f"ğŸ¯ç±»åˆ«: {final_label}")
                print(f"ç†ç”±: {final_result}")
                if SHOW_SAMPLE:
                    print(f"ç›¸ä¼¼{'è¯ˆéª—' if final_label != 'æ­£å¸¸çŸ­ä¿¡' else ''}çŸ­ä¿¡: {similar_sms}")
                return final_result[0]

# ========== ä¸»å‡½æ•° ==========
if __name__ == "__main__":
    if SAFE_MODE:
        print('''
        å½“å‰æ¨¡å¼: ç¨³å¦¥æ¨¡å¼
        è¯´æ˜ï¼šåœ¨å½“å‰æ¨¡å¼ä¸‹ï¼Œæ¯æ¡çŸ­ä¿¡éƒ½ä¼šè¢«è‡³å°‘ç”„åˆ«ä¸¤æ¬¡ï¼Œè‹¥ä¸¤æ¬¡ç»“æœç›¸åŒï¼Œåˆ™è¾“å‡ºï¼Œå¦åˆ™è¿›è¡Œç¬¬ä¸‰æ¬¡ç”„åˆ«ï¼Œä»¥å¤šæ•°ç»“æœä¸ºå‡†ï¼Œè‹¥æ— å¤šæ•°ç»“æœï¼Œåˆ™æç¤ºæ— æ³•åˆ¤æ–­
            åœ¨ç›®å‰çš„æç¤ºè¯ä¸‹ï¼Œæ¨¡å‹è¡¨ç°ç¨³å®šï¼Œè‹¥ä¸å¼€å¯é‡‡æ ·ï¼Œåˆ™ç¨³å¦¥æ¨¡å¼å’Œæ™®é€šæ¨¡å¼è¡¨ç°å‡ ä¹æ— å·®
        ''')
    else:
        print('''
        å½“å‰æ¨¡å¼: æ™®é€šæ¨¡å¼
        è¯´æ˜ï¼šæ¯æ¡çŸ­ä¿¡è¿›è¡Œä¸€æ¬¡ç”„åˆ«å¹¶è¾“å‡ºç»“æœï¼Œè‹¥æ¨¡å‹è¾“å‡ºæ ¼å¼å¼‚å¸¸åˆ™ä¼šè¿›è¡Œé‡è¯•ã€‚
                ''')
    while True:
        sms = input("\nğŸ“¨ è¯·è¾“å…¥è¦åˆ†ç±»çš„çŸ­ä¿¡ï¼š")
        if len(sms) == 0:
            print("è¯·è¾“å…¥å†…å®¹ï¼Œä¸è¦è¾“å…¥ç©ºä¿¡æ¯ã€‚\n")
            continue
        classify_sms(sms)